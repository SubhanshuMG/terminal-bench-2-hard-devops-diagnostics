{
    "results": {
        "tool": {
            "name": "pytest",
            "version": "8.4.1"
        },
        "summary": {
            "tests": 18,
            "passed": 15,
            "failed": 3,
            "skipped": 0,
            "pending": 0,
            "other": 0,
            "start": 1772223495.6431737,
            "stop": 1772223495.6641104
        },
        "tests": [
            {
                "name": "test_outputs.py::test_report_file_exists",
                "status": "passed",
                "duration": 0.0001856260005297372,
                "start": 1772223495.652328,
                "stop": 1772223495.652654,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_report_top_level_keys",
                "status": "passed",
                "duration": 0.0001813750004657777,
                "start": 1772223495.6527314,
                "stop": 1772223495.6529927,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_deployment_name",
                "status": "passed",
                "duration": 0.00010299900077370694,
                "start": 1772223495.6530533,
                "stop": 1772223495.6532278,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_all_five_services_present",
                "status": "passed",
                "duration": 0.00010062500041385647,
                "start": 1772223495.653283,
                "stop": 1772223495.6534503,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_auth_service_healthy",
                "status": "passed",
                "duration": 9.625000075175194e-05,
                "start": 1772223495.6535034,
                "stop": 1772223495.653664,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_api_gateway_healthy",
                "status": "passed",
                "duration": 0.00010470799952599918,
                "start": 1772223495.6537175,
                "stop": 1772223495.6538844,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_cache_service_healthy",
                "status": "passed",
                "duration": 9.287500142818317e-05,
                "start": 1772223495.6539361,
                "stop": 1772223495.654092,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_worker_service_unhealthy",
                "status": "failed",
                "raw_status": "call_failed",
                "duration": 0.0002379159996053204,
                "start": 1772223495.6541426,
                "stop": 1772223495.659238,
                "retries": 0,
                "file_path": "test_outputs.py",
                "trace": "report = {'critical_services_healthy': True, 'deployment_name': 'production-stack', 'overall_status': 'healthy', 'readiness_score': 1.0, ...}\n\n    def test_worker_service_unhealthy(report):\n        \"\"\"worker-service returns HTTP 200 but with status='degraded' in body; must be reported unhealthy.\"\"\"\n        svc = report[\"service_statuses\"][\"worker-service\"]\n>       assert svc[\"status\"] == \"unhealthy\", (\n            \"worker-service reports {'status': 'degraded'} in its JSON body. \"\n            \"Despite HTTP 200, a 'degraded' body status means the service is unhealthy.\"\n        )\nE       AssertionError: worker-service reports {'status': 'degraded'} in its JSON body. Despite HTTP 200, a 'degraded' body status means the service is unhealthy.\nE       assert 'healthy' == 'unhealthy'\nE         \nE         - unhealthy\nE         ? --\nE         + healthy\n\n/tests/test_outputs.py:93: AssertionError",
                "message": "The test failed in the call phase due to an assertion error"
            },
            {
                "name": "test_outputs.py::test_notification_service_healthy",
                "status": "passed",
                "duration": 0.00013620799927593907,
                "start": 1772223495.6593053,
                "stop": 1772223495.6595407,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_service_criticality_values",
                "status": "passed",
                "duration": 0.00014525000096909935,
                "start": 1772223495.6596298,
                "stop": 1772223495.659889,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_readiness_score",
                "status": "failed",
                "raw_status": "call_failed",
                "duration": 0.00015929100027278764,
                "start": 1772223495.6599615,
                "stop": 1772223495.6612396,
                "retries": 0,
                "file_path": "test_outputs.py",
                "trace": "report = {'critical_services_healthy': True, 'deployment_name': 'production-stack', 'overall_status': 'healthy', 'readiness_score': 1.0, ...}\n\n    def test_readiness_score(report):\n        \"\"\"\n        Weighted score with high=3, medium=2, low=1:\n          healthy:  auth(3) + api-gateway(3) + cache(2) + notification(1) = 9\n          total:    9 + worker(1) = 10\n          score:    9/10 = 0.9\n        \"\"\"\n>       assert abs(report[\"readiness_score\"] - 0.9) < 0.001, (\n            f\"Expected readiness_score ~0.9, got {report['readiness_score']}. \"\n            \"Check criticality weights: high=3, medium=2, low=1.\"\n        )\nE       AssertionError: Expected readiness_score ~0.9, got 1.0. Check criticality weights: high=3, medium=2, low=1.\nE       assert 0.09999999999999998 < 0.001\nE        +  where 0.09999999999999998 = abs((1.0 - 0.9))\n\n/tests/test_outputs.py:132: AssertionError",
                "message": "The test failed in the call phase due to an assertion error"
            },
            {
                "name": "test_outputs.py::test_critical_services_healthy",
                "status": "passed",
                "duration": 9.904100079438649e-05,
                "start": 1772223495.6613011,
                "stop": 1772223495.6614926,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_overall_status_degraded",
                "status": "failed",
                "raw_status": "call_failed",
                "duration": 0.0001753329997882247,
                "start": 1772223495.6615462,
                "stop": 1772223495.6628034,
                "retries": 0,
                "file_path": "test_outputs.py",
                "trace": "report = {'critical_services_healthy': True, 'deployment_name': 'production-stack', 'overall_status': 'healthy', 'readiness_score': 1.0, ...}\n\n    def test_overall_status_degraded(report):\n        \"\"\"\n        All high-criticality services are healthy but score (0.9) < 0.95,\n        so overall_status must be 'degraded'.\n        \"\"\"\n>       assert report[\"overall_status\"] == \"degraded\", (\n            f\"Expected overall_status='degraded', got '{report['overall_status']}'. \"\n            \"Rules: critical_services_healthy=True + score<0.95 -> 'degraded'.\"\n        )\nE       AssertionError: Expected overall_status='degraded', got 'healthy'. Rules: critical_services_healthy=True + score<0.95 -> 'degraded'.\nE       assert 'healthy' == 'degraded'\nE         \nE         - degraded\nE         + healthy\n\n/tests/test_outputs.py:159: AssertionError",
                "message": "The test failed in the call phase due to an assertion error"
            },
            {
                "name": "test_outputs.py::test_startup_order_has_all_services",
                "status": "passed",
                "duration": 0.00010687400026654359,
                "start": 1772223495.6628695,
                "stop": 1772223495.663059,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_startup_order_auth_before_gateway",
                "status": "passed",
                "duration": 9.604100068827393e-05,
                "start": 1772223495.6631186,
                "stop": 1772223495.6632936,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_startup_order_cache_before_gateway",
                "status": "passed",
                "duration": 9.2208999376453e-05,
                "start": 1772223495.6633468,
                "stop": 1772223495.6635149,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_startup_order_gateway_before_worker",
                "status": "passed",
                "duration": 8.958400030678604e-05,
                "start": 1772223495.6635656,
                "stop": 1772223495.663731,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_startup_order_worker_before_notification",
                "status": "passed",
                "duration": 0.00011424999956943793,
                "start": 1772223495.6637967,
                "stop": 1772223495.6639874,
                "retries": 0,
                "file_path": "test_outputs.py"
            }
        ]
    }
}