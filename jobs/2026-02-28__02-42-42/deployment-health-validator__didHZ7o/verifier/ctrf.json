{
    "results": {
        "tool": {
            "name": "pytest",
            "version": "8.4.1"
        },
        "summary": {
            "tests": 18,
            "passed": 15,
            "failed": 3,
            "skipped": 0,
            "pending": 0,
            "other": 0,
            "start": 1772226809.5617914,
            "stop": 1772226809.5825512
        },
        "tests": [
            {
                "name": "test_outputs.py::test_report_file_exists",
                "status": "passed",
                "duration": 0.0001847079984145239,
                "start": 1772226809.5704582,
                "stop": 1772226809.570806,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_report_top_level_keys",
                "status": "passed",
                "duration": 0.00017991700042330194,
                "start": 1772226809.5708838,
                "stop": 1772226809.5711706,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_deployment_name",
                "status": "passed",
                "duration": 0.00012537599832285196,
                "start": 1772226809.5712533,
                "stop": 1772226809.5714977,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_all_five_services_present",
                "status": "passed",
                "duration": 0.00010079200001200661,
                "start": 1772226809.5715544,
                "stop": 1772226809.5717354,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_auth_service_healthy",
                "status": "passed",
                "duration": 0.0001031259998853784,
                "start": 1772226809.5717967,
                "stop": 1772226809.5720005,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_api_gateway_healthy",
                "status": "passed",
                "duration": 0.00010025099982158281,
                "start": 1772226809.572053,
                "stop": 1772226809.5722399,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_cache_service_healthy",
                "status": "passed",
                "duration": 0.00010516700058360584,
                "start": 1772226809.5722964,
                "stop": 1772226809.5724802,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_worker_service_unhealthy",
                "status": "failed",
                "raw_status": "call_failed",
                "duration": 0.00021466799989866558,
                "start": 1772226809.5725422,
                "stop": 1772226809.5776277,
                "retries": 0,
                "file_path": "test_outputs.py",
                "trace": "report = {'critical_services_healthy': True, 'deployment_name': 'production-stack', 'overall_status': 'healthy', 'readiness_score': 1.0, ...}\n\n    def test_worker_service_unhealthy(report):\n        \"\"\"worker-service returns HTTP 200 but with status='degraded' in body; must be reported unhealthy.\"\"\"\n        svc = report[\"service_statuses\"][\"worker-service\"]\n>       assert svc[\"status\"] == \"unhealthy\", (\n            \"worker-service reports {'status': 'degraded'} in its JSON body. \"\n            \"Despite HTTP 200, a 'degraded' body status means the service is unhealthy.\"\n        )\nE       AssertionError: worker-service reports {'status': 'degraded'} in its JSON body. Despite HTTP 200, a 'degraded' body status means the service is unhealthy.\nE       assert 'healthy' == 'unhealthy'\nE         \nE         - unhealthy\nE         ? --\nE         + healthy\n\n/tests/test_outputs.py:93: AssertionError",
                "message": "The test failed in the call phase due to an assertion error"
            },
            {
                "name": "test_outputs.py::test_notification_service_healthy",
                "status": "passed",
                "duration": 0.00010445800035085995,
                "start": 1772226809.5776932,
                "stop": 1772226809.5778973,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_service_criticality_values",
                "status": "passed",
                "duration": 0.00010712599942053203,
                "start": 1772226809.5779495,
                "stop": 1772226809.5781498,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_readiness_score",
                "status": "failed",
                "raw_status": "call_failed",
                "duration": 0.00014920900139259174,
                "start": 1772226809.5782065,
                "stop": 1772226809.579557,
                "retries": 0,
                "file_path": "test_outputs.py",
                "trace": "report = {'critical_services_healthy': True, 'deployment_name': 'production-stack', 'overall_status': 'healthy', 'readiness_score': 1.0, ...}\n\n    def test_readiness_score(report):\n        \"\"\"\n        Weighted score with high=3, medium=2, low=1:\n          healthy:  auth(3) + api-gateway(3) + cache(2) + notification(1) = 9\n          total:    9 + worker(1) = 10\n          score:    9/10 = 0.9\n        \"\"\"\n>       assert abs(report[\"readiness_score\"] - 0.9) < 0.001, (\n            f\"Expected readiness_score ~0.9, got {report['readiness_score']}. \"\n            \"Check criticality weights: high=3, medium=2, low=1.\"\n        )\nE       AssertionError: Expected readiness_score ~0.9, got 1.0. Check criticality weights: high=3, medium=2, low=1.\nE       assert 0.09999999999999998 < 0.001\nE        +  where 0.09999999999999998 = abs((1.0 - 0.9))\n\n/tests/test_outputs.py:132: AssertionError",
                "message": "The test failed in the call phase due to an assertion error"
            },
            {
                "name": "test_outputs.py::test_critical_services_healthy",
                "status": "passed",
                "duration": 0.00010408300113340374,
                "start": 1772226809.5796177,
                "stop": 1772226809.5798118,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_overall_status_degraded",
                "status": "failed",
                "raw_status": "call_failed",
                "duration": 0.0001577909988554893,
                "start": 1772226809.5798693,
                "stop": 1772226809.5811424,
                "retries": 0,
                "file_path": "test_outputs.py",
                "trace": "report = {'critical_services_healthy': True, 'deployment_name': 'production-stack', 'overall_status': 'healthy', 'readiness_score': 1.0, ...}\n\n    def test_overall_status_degraded(report):\n        \"\"\"\n        All high-criticality services are healthy but score (0.9) < 0.95,\n        so overall_status must be 'degraded'.\n        \"\"\"\n>       assert report[\"overall_status\"] == \"degraded\", (\n            f\"Expected overall_status='degraded', got '{report['overall_status']}'. \"\n            \"Rules: critical_services_healthy=True + score<0.95 -> 'degraded'.\"\n        )\nE       AssertionError: Expected overall_status='degraded', got 'healthy'. Rules: critical_services_healthy=True + score<0.95 -> 'degraded'.\nE       assert 'healthy' == 'degraded'\nE         \nE         - degraded\nE         + healthy\n\n/tests/test_outputs.py:159: AssertionError",
                "message": "The test failed in the call phase due to an assertion error"
            },
            {
                "name": "test_outputs.py::test_startup_order_has_all_services",
                "status": "passed",
                "duration": 0.00012675000107265078,
                "start": 1772226809.5812068,
                "stop": 1772226809.5814524,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_startup_order_auth_before_gateway",
                "status": "passed",
                "duration": 0.0001262500009033829,
                "start": 1772226809.5815306,
                "stop": 1772226809.5817492,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_startup_order_cache_before_gateway",
                "status": "passed",
                "duration": 9.77080017037224e-05,
                "start": 1772226809.5818021,
                "stop": 1772226809.5819883,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_startup_order_gateway_before_worker",
                "status": "passed",
                "duration": 9.362499986309558e-05,
                "start": 1772226809.5820458,
                "stop": 1772226809.5822203,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_startup_order_worker_before_notification",
                "status": "passed",
                "duration": 9.837499965215102e-05,
                "start": 1772226809.582274,
                "stop": 1772226809.5824466,
                "retries": 0,
                "file_path": "test_outputs.py"
            }
        ]
    }
}