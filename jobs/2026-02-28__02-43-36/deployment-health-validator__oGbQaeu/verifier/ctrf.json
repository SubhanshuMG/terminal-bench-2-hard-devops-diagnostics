{
    "results": {
        "tool": {
            "name": "pytest",
            "version": "8.4.1"
        },
        "summary": {
            "tests": 18,
            "passed": 15,
            "failed": 3,
            "skipped": 0,
            "pending": 0,
            "other": 0,
            "start": 1772226866.8210204,
            "stop": 1772226866.841088
        },
        "tests": [
            {
                "name": "test_outputs.py::test_report_file_exists",
                "status": "passed",
                "duration": 0.00019966800027759746,
                "start": 1772226866.8293536,
                "stop": 1772226866.8297224,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_report_top_level_keys",
                "status": "passed",
                "duration": 0.00019295700076327194,
                "start": 1772226866.8298116,
                "stop": 1772226866.8301072,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_deployment_name",
                "status": "passed",
                "duration": 0.00011104099939984735,
                "start": 1772226866.8301847,
                "stop": 1772226866.8304083,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_all_five_services_present",
                "status": "passed",
                "duration": 0.00011166699914610945,
                "start": 1772226866.8304656,
                "stop": 1772226866.830666,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_auth_service_healthy",
                "status": "passed",
                "duration": 9.424999916518573e-05,
                "start": 1772226866.8307269,
                "stop": 1772226866.8309016,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_api_gateway_healthy",
                "status": "passed",
                "duration": 9.15830005396856e-05,
                "start": 1772226866.8309546,
                "stop": 1772226866.831124,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_cache_service_healthy",
                "status": "passed",
                "duration": 9.791599950403906e-05,
                "start": 1772226866.8311777,
                "stop": 1772226866.831354,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_worker_service_unhealthy",
                "status": "failed",
                "raw_status": "call_failed",
                "duration": 0.00022091600112617016,
                "start": 1772226866.8314047,
                "stop": 1772226866.8362532,
                "retries": 0,
                "file_path": "test_outputs.py",
                "trace": "report = {'critical_services_healthy': True, 'deployment_name': 'production-stack', 'overall_status': 'healthy', 'readiness_score': 1.0, ...}\n\n    def test_worker_service_unhealthy(report):\n        \"\"\"worker-service returns HTTP 200 but with status='degraded' in body; must be reported unhealthy.\"\"\"\n        svc = report[\"service_statuses\"][\"worker-service\"]\n>       assert svc[\"status\"] == \"unhealthy\", (\n            \"worker-service reports {'status': 'degraded'} in its JSON body. \"\n            \"Despite HTTP 200, a 'degraded' body status means the service is unhealthy.\"\n        )\nE       AssertionError: worker-service reports {'status': 'degraded'} in its JSON body. Despite HTTP 200, a 'degraded' body status means the service is unhealthy.\nE       assert 'healthy' == 'unhealthy'\nE         \nE         - unhealthy\nE         ? --\nE         + healthy\n\n/tests/test_outputs.py:93: AssertionError",
                "message": "The test failed in the call phase due to an assertion error"
            },
            {
                "name": "test_outputs.py::test_notification_service_healthy",
                "status": "passed",
                "duration": 0.0001105840001400793,
                "start": 1772226866.8363264,
                "stop": 1772226866.836534,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_service_criticality_values",
                "status": "passed",
                "duration": 9.420900096301921e-05,
                "start": 1772226866.836589,
                "stop": 1772226866.8367767,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_readiness_score",
                "status": "failed",
                "raw_status": "call_failed",
                "duration": 0.00014287399972090498,
                "start": 1772226866.8368278,
                "stop": 1772226866.8381834,
                "retries": 0,
                "file_path": "test_outputs.py",
                "trace": "report = {'critical_services_healthy': True, 'deployment_name': 'production-stack', 'overall_status': 'healthy', 'readiness_score': 1.0, ...}\n\n    def test_readiness_score(report):\n        \"\"\"\n        Weighted score with high=3, medium=2, low=1:\n          healthy:  auth(3) + api-gateway(3) + cache(2) + notification(1) = 9\n          total:    9 + worker(1) = 10\n          score:    9/10 = 0.9\n        \"\"\"\n>       assert abs(report[\"readiness_score\"] - 0.9) < 0.001, (\n            f\"Expected readiness_score ~0.9, got {report['readiness_score']}. \"\n            \"Check criticality weights: high=3, medium=2, low=1.\"\n        )\nE       AssertionError: Expected readiness_score ~0.9, got 1.0. Check criticality weights: high=3, medium=2, low=1.\nE       assert 0.09999999999999998 < 0.001\nE        +  where 0.09999999999999998 = abs((1.0 - 0.9))\n\n/tests/test_outputs.py:132: AssertionError",
                "message": "The test failed in the call phase due to an assertion error"
            },
            {
                "name": "test_outputs.py::test_critical_services_healthy",
                "status": "passed",
                "duration": 0.00010216700138698798,
                "start": 1772226866.838252,
                "stop": 1772226866.8384438,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_overall_status_degraded",
                "status": "failed",
                "raw_status": "call_failed",
                "duration": 0.00016170800154213794,
                "start": 1772226866.8384964,
                "stop": 1772226866.8397448,
                "retries": 0,
                "file_path": "test_outputs.py",
                "trace": "report = {'critical_services_healthy': True, 'deployment_name': 'production-stack', 'overall_status': 'healthy', 'readiness_score': 1.0, ...}\n\n    def test_overall_status_degraded(report):\n        \"\"\"\n        All high-criticality services are healthy but score (0.9) < 0.95,\n        so overall_status must be 'degraded'.\n        \"\"\"\n>       assert report[\"overall_status\"] == \"degraded\", (\n            f\"Expected overall_status='degraded', got '{report['overall_status']}'. \"\n            \"Rules: critical_services_healthy=True + score<0.95 -> 'degraded'.\"\n        )\nE       AssertionError: Expected overall_status='degraded', got 'healthy'. Rules: critical_services_healthy=True + score<0.95 -> 'degraded'.\nE       assert 'healthy' == 'degraded'\nE         \nE         - degraded\nE         + healthy\n\n/tests/test_outputs.py:159: AssertionError",
                "message": "The test failed in the call phase due to an assertion error"
            },
            {
                "name": "test_outputs.py::test_startup_order_has_all_services",
                "status": "passed",
                "duration": 9.812400094233453e-05,
                "start": 1772226866.8398056,
                "stop": 1772226866.8399892,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_startup_order_auth_before_gateway",
                "status": "passed",
                "duration": 0.00011754200204450171,
                "start": 1772226866.8400402,
                "stop": 1772226866.8402529,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_startup_order_cache_before_gateway",
                "status": "passed",
                "duration": 9.787499948288314e-05,
                "start": 1772226866.8403091,
                "stop": 1772226866.840493,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_startup_order_gateway_before_worker",
                "status": "passed",
                "duration": 9.324900020146742e-05,
                "start": 1772226866.840545,
                "stop": 1772226866.8407266,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_startup_order_worker_before_notification",
                "status": "passed",
                "duration": 0.00010104200009664055,
                "start": 1772226866.840779,
                "stop": 1772226866.8409624,
                "retries": 0,
                "file_path": "test_outputs.py"
            }
        ]
    }
}