{
    "results": {
        "tool": {
            "name": "pytest",
            "version": "8.4.1"
        },
        "summary": {
            "tests": 18,
            "passed": 15,
            "failed": 3,
            "skipped": 0,
            "pending": 0,
            "other": 0,
            "start": 1772227062.983981,
            "stop": 1772227063.0075498
        },
        "tests": [
            {
                "name": "test_outputs.py::test_report_file_exists",
                "status": "passed",
                "duration": 0.000269748999926378,
                "start": 1772227062.9935908,
                "stop": 1772227062.9940658,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_report_top_level_keys",
                "status": "passed",
                "duration": 0.00021966700114717241,
                "start": 1772227062.9942055,
                "stop": 1772227062.9946318,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_deployment_name",
                "status": "passed",
                "duration": 0.00015675000031478703,
                "start": 1772227062.9947174,
                "stop": 1772227062.9949694,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_all_five_services_present",
                "status": "passed",
                "duration": 0.00018575000103737693,
                "start": 1772227062.9950902,
                "stop": 1772227062.9954338,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_auth_service_healthy",
                "status": "passed",
                "duration": 0.0001392499980283901,
                "start": 1772227062.995542,
                "stop": 1772227062.9957979,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_api_gateway_healthy",
                "status": "passed",
                "duration": 0.00011258300037297886,
                "start": 1772227062.9958653,
                "stop": 1772227062.9960701,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_cache_service_healthy",
                "status": "passed",
                "duration": 0.00010891599777096417,
                "start": 1772227062.9961321,
                "stop": 1772227062.996359,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_worker_service_unhealthy",
                "status": "failed",
                "raw_status": "call_failed",
                "duration": 0.0002600419993541436,
                "start": 1772227062.9964223,
                "stop": 1772227063.0018477,
                "retries": 0,
                "file_path": "test_outputs.py",
                "trace": "report = {'critical_services_healthy': True, 'deployment_name': 'production-stack', 'overall_status': 'healthy', 'readiness_score': 1.0, ...}\n\n    def test_worker_service_unhealthy(report):\n        \"\"\"worker-service returns HTTP 200 but with status='degraded' in body; must be reported unhealthy.\"\"\"\n        svc = report[\"service_statuses\"][\"worker-service\"]\n>       assert svc[\"status\"] == \"unhealthy\", (\n            \"worker-service reports {'status': 'degraded'} in its JSON body. \"\n            \"Despite HTTP 200, a 'degraded' body status means the service is unhealthy.\"\n        )\nE       AssertionError: worker-service reports {'status': 'degraded'} in its JSON body. Despite HTTP 200, a 'degraded' body status means the service is unhealthy.\nE       assert 'healthy' == 'unhealthy'\nE         \nE         - unhealthy\nE         ? --\nE         + healthy\n\n/tests/test_outputs.py:93: AssertionError",
                "message": "The test failed in the call phase due to an assertion error"
            },
            {
                "name": "test_outputs.py::test_notification_service_healthy",
                "status": "passed",
                "duration": 0.0001174990029539913,
                "start": 1772227063.0019383,
                "stop": 1772227063.002155,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_service_criticality_values",
                "status": "passed",
                "duration": 0.00013479100016411394,
                "start": 1772227063.0022297,
                "stop": 1772227063.0024574,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_readiness_score",
                "status": "failed",
                "raw_status": "call_failed",
                "duration": 0.00018904100033978466,
                "start": 1772227063.0025449,
                "stop": 1772227063.0041666,
                "retries": 0,
                "file_path": "test_outputs.py",
                "trace": "report = {'critical_services_healthy': True, 'deployment_name': 'production-stack', 'overall_status': 'healthy', 'readiness_score': 1.0, ...}\n\n    def test_readiness_score(report):\n        \"\"\"\n        Weighted score with high=3, medium=2, low=1:\n          healthy:  auth(3) + api-gateway(3) + cache(2) + notification(1) = 9\n          total:    9 + worker(1) = 10\n          score:    9/10 = 0.9\n        \"\"\"\n>       assert abs(report[\"readiness_score\"] - 0.9) < 0.001, (\n            f\"Expected readiness_score ~0.9, got {report['readiness_score']}. \"\n            \"Check criticality weights: high=3, medium=2, low=1.\"\n        )\nE       AssertionError: Expected readiness_score ~0.9, got 1.0. Check criticality weights: high=3, medium=2, low=1.\nE       assert 0.09999999999999998 < 0.001\nE        +  where 0.09999999999999998 = abs((1.0 - 0.9))\n\n/tests/test_outputs.py:132: AssertionError",
                "message": "The test failed in the call phase due to an assertion error"
            },
            {
                "name": "test_outputs.py::test_critical_services_healthy",
                "status": "passed",
                "duration": 0.00014074999853619374,
                "start": 1772227063.0042548,
                "stop": 1772227063.0044954,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_overall_status_degraded",
                "status": "failed",
                "raw_status": "call_failed",
                "duration": 0.00021066699991934001,
                "start": 1772227063.0045857,
                "stop": 1772227063.006052,
                "retries": 0,
                "file_path": "test_outputs.py",
                "trace": "report = {'critical_services_healthy': True, 'deployment_name': 'production-stack', 'overall_status': 'healthy', 'readiness_score': 1.0, ...}\n\n    def test_overall_status_degraded(report):\n        \"\"\"\n        All high-criticality services are healthy but score (0.9) < 0.95,\n        so overall_status must be 'degraded'.\n        \"\"\"\n>       assert report[\"overall_status\"] == \"degraded\", (\n            f\"Expected overall_status='degraded', got '{report['overall_status']}'. \"\n            \"Rules: critical_services_healthy=True + score<0.95 -> 'degraded'.\"\n        )\nE       AssertionError: Expected overall_status='degraded', got 'healthy'. Rules: critical_services_healthy=True + score<0.95 -> 'degraded'.\nE       assert 'healthy' == 'degraded'\nE         \nE         - degraded\nE         + healthy\n\n/tests/test_outputs.py:159: AssertionError",
                "message": "The test failed in the call phase due to an assertion error"
            },
            {
                "name": "test_outputs.py::test_startup_order_has_all_services",
                "status": "passed",
                "duration": 0.00013816700084134936,
                "start": 1772227063.006126,
                "stop": 1772227063.0063589,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_startup_order_auth_before_gateway",
                "status": "passed",
                "duration": 0.00011279099999228492,
                "start": 1772227063.00643,
                "stop": 1772227063.0066285,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_startup_order_cache_before_gateway",
                "status": "passed",
                "duration": 0.00010983399988617748,
                "start": 1772227063.006693,
                "stop": 1772227063.0068872,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_startup_order_gateway_before_worker",
                "status": "passed",
                "duration": 0.00011404100223444402,
                "start": 1772227063.0069501,
                "stop": 1772227063.0071507,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_startup_order_worker_before_notification",
                "status": "passed",
                "duration": 0.00011899900164280552,
                "start": 1772227063.0072334,
                "stop": 1772227063.0074327,
                "retries": 0,
                "file_path": "test_outputs.py"
            }
        ]
    }
}